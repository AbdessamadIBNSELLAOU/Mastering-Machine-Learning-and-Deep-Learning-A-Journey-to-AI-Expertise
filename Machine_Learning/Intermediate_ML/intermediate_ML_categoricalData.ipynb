{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb5a5bd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A categorical variable takes only a limited number of values.\n",
    "\n",
    "    Consider a survey that asks how often you eat breakfast and provides four options: \"Never\", \"Rarely\", \"Most days\", or \"Every day\". In this case, the data is categorical, because responses fall into a fixed set of categories.\n",
    "    \n",
    "    If people responded to a survey about which what brand of car they owned, the responses would fall into categories like \"Honda\", \"Toyota\", and \"Ford\". In this case, the data is also categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a863679",
   "metadata": {},
   "source": [
    "You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first. In this tutorial, we'll compare three approaches that you can use to prepare your categorical data.\n",
    "\n",
    "## Three Approaches\n",
    "1) Drop Categorical Variables\n",
    "2) Ordinal Encoding: assigns each unique value to a different integer.\n",
    "3) One-Hot Encoding: One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0db121f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>11694</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NridgHt</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>6600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NAmes</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>13360</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>13265</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Mitchel</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>13704</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\n",
       "Id                                                                        \n",
       "619          20       RL    11694   Pave      Reg         Lvl    AllPub   \n",
       "871          20       RL     6600   Pave      Reg         Lvl    AllPub   \n",
       "93           30       RL    13360   Pave      IR1         HLS    AllPub   \n",
       "818          20       RL    13265   Pave      IR1         Lvl    AllPub   \n",
       "303          20       RL    13704   Pave      IR1         Lvl    AllPub   \n",
       "\n",
       "    LotConfig LandSlope Neighborhood  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\n",
       "Id                                    ...                                       \n",
       "619    Inside       Gtl      NridgHt  ...         108             0         0   \n",
       "871    Inside       Gtl        NAmes  ...           0             0         0   \n",
       "93     Inside       Gtl      Crawfor  ...           0            44         0   \n",
       "818   CulDSac       Gtl      Mitchel  ...          59             0         0   \n",
       "303    Corner       Gtl      CollgCr  ...          81             0         0   \n",
       "\n",
       "    ScreenPorch  PoolArea  MiscVal  MoSold  YrSold SaleType SaleCondition  \n",
       "Id                                                                         \n",
       "619         260         0        0       7    2007      New       Partial  \n",
       "871           0         0        0       8    2009       WD        Normal  \n",
       "93            0         0        0       8    2009       WD        Normal  \n",
       "818           0         0        0       7    2008       WD        Normal  \n",
       "303           0         0        0       1    2006       WD        Normal  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X = pd.read_csv('train_CV.csv', index_col='Id') \n",
    "X_test = pd.read_csv('test_CV.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice\n",
    "X.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll drop columns with missing values\n",
    "cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n",
    "X.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_test.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                      train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1034e",
   "metadata": {},
   "source": [
    "Notice that the dataset contains both numerical and categorical variables. We'll need to encode the categorical data before training a model.\n",
    "\n",
    "To compare different models, we'll use score_dataset() function that reports the mean absolute error (MAE) from a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa6bd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a0164",
   "metadata": {},
   "source": [
    "## Step 1: Drop columns with categorical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "425d34ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "17837.82570776256\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49a9a4",
   "metadata": {},
   "source": [
    "Before jumping into ordinal encoding, we'll investigate the dataset. Specifically, we'll look at the 'Condition2' column. The code cell below prints the unique entries in both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fe56ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Condition2' column in training data: ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n",
      "\n",
      "Unique values in 'Condition2' column in validation data: ['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\n",
    "print(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49a2e1",
   "metadata": {},
   "source": [
    "## Step 2: Ordinal encoding\n",
    "\n",
    "### Part A\n",
    "\n",
    "If we now write code to: \n",
    "- fit an ordinal encoder to the training data, and then \n",
    "- use it to transform both the training and validation data, \n",
    "\n",
    "we'll get an error.  Can you see why this is the case? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d0dc3",
   "metadata": {},
   "source": [
    "Answer: \n",
    "    \n",
    "    Fitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them. Notice that the 'Condition2' column in the validation data contains the values 'RRAn' and 'RRNn', but these don't appear in the training data -- thus, if we try to use an ordinal encoder with scikit-learn, the code will throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068c85f",
   "metadata": {},
   "source": [
    " Now let's save the problematic columns to a Python list bad_label_cols. \n",
    " \n",
    " Likewise, columns that can be safely ordinal encoded are stored in good_label_cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "529fc24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be ordinal encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Functional', 'Condition2', 'RoofMatl']\n"
     ]
    }
   ],
   "source": [
    "# Categorical columns in the training data\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63315dbb",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "- Drop the categorical columns in `bad_label_cols` from the dataset. \n",
    "- Ordinal encode the categorical columns in `good_label_cols`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "503689a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Drop categorical columns that will not be encoded\n",
    "label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "# Apply ordinal encoder \n",
    "OE = OrdinalEncoder() \n",
    "label_X_train[good_label_cols] = OE.fit_transform(label_X_train[good_label_cols])\n",
    "label_X_valid[good_label_cols] = OE.transform(label_X_valid[good_label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb6d3ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Ordinal Encoding):\n",
      "17098.01649543379\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8efd444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Street', 2),\n",
       " ('Utilities', 2),\n",
       " ('CentralAir', 2),\n",
       " ('LandSlope', 3),\n",
       " ('PavedDrive', 3),\n",
       " ('LotShape', 4),\n",
       " ('LandContour', 4),\n",
       " ('ExterQual', 4),\n",
       " ('KitchenQual', 4),\n",
       " ('MSZoning', 5),\n",
       " ('LotConfig', 5),\n",
       " ('BldgType', 5),\n",
       " ('ExterCond', 5),\n",
       " ('HeatingQC', 5),\n",
       " ('Condition2', 6),\n",
       " ('RoofStyle', 6),\n",
       " ('Foundation', 6),\n",
       " ('Heating', 6),\n",
       " ('Functional', 6),\n",
       " ('SaleCondition', 6),\n",
       " ('RoofMatl', 7),\n",
       " ('HouseStyle', 8),\n",
       " ('Condition1', 9),\n",
       " ('SaleType', 9),\n",
       " ('Exterior1st', 15),\n",
       " ('Exterior2nd', 16),\n",
       " ('Neighborhood', 25)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6b80a",
   "metadata": {},
   "source": [
    "## Step 3: Investigating cardinality\n",
    "\n",
    "### Part A\n",
    "\n",
    "The output above shows, for each column with categorical data, the number of unique values in the column.  For instance, the `'Street'` column in the training data has two unique values: `'Grvl'` and `'Pave'`, corresponding to a gravel road and a paved road, respectively.\n",
    "\n",
    "We refer to the number of unique entries of a categorical variable as the **cardinality** of that categorical variable.  For instance, the `'Street'` variable has cardinality 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336674d7",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use ordinal encoding.\n",
    "\n",
    "As an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.  \n",
    "- If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?  \n",
    "- If we instead replace the column with the ordinal encoding, how many entries are added? \n",
    "\n",
    "- --> OH_entries_added = 10000*99\n",
    "- --> label_entries_added = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a02a1a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that will be one-hot encoded: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "\n",
      "Categorical columns that will be dropped from the dataset: ['Neighborhood', 'Exterior2nd', 'Exterior1st']\n"
     ]
    }
   ],
   "source": [
    "# Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2368a",
   "metadata": {},
   "source": [
    "## Step 4: One-hot encoding\n",
    "\n",
    "One-hot encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `OH_X_train` and `OH_X_valid`, respectively.  \n",
    "- The full list of categorical columns in the dataset can be found in the Python list `object_cols`.\n",
    "- Only one-hot encode the categorical columns in `low_cardinality_cols`.  All other categorical columns should be dropped from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1e32f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Cols to drop\n",
    "cols_to_drop = list(set(object_cols) - set(low_cardinality_cols))\n",
    "\n",
    "# keep only columns that exist in the dataframes X_train and X_valid\n",
    "cols_to_drop_existing_train = [col for col in cols_to_drop if col in X_train.columns]\n",
    "cols_to_drop_existing_valid = [col for col in cols_to_drop if col in X_valid.columns]\n",
    "\n",
    "# Drop categorical columns that will not be encoded\n",
    "X_train.drop(columns=cols_to_drop_existing_train, axis=1, inplace=True)\n",
    "X_valid.drop(columns=cols_to_drop_existing_valid, axis=1, inplace=True)\n",
    "OH_Encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "OH_X_train = pd.DataFrame(OH_Encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_X_valid = pd.DataFrame(OH_Encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "#add index that has been removed due to oh_encoder\n",
    "OH_X_train.index = X_train.index\n",
    "OH_X_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(low_cardinality_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n",
    "\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_X_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_X_valid], axis=1)\n",
    "\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "432f131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "17525.345719178084\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "400e2a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (Your approach):\n",
      "17525.345719178084\n"
     ]
    }
   ],
   "source": [
    "# Define and fit model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(OH_X_train, y_train)\n",
    "\n",
    "# Get validation predictions and MAE\n",
    "preds_valid = model.predict(OH_X_valid)\n",
    "print(\"MAE (Your approach):\")\n",
    "print(mean_absolute_error(y_valid, preds_valid))\n",
    "\n",
    "# Save test predictions to file\n",
    "output = pd.DataFrame({'Id': X_valid.index,\n",
    "                       'SalePrice': preds_valid})\n",
    "output.to_csv('submission2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
