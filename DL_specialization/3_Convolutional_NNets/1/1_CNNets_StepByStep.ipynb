{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09320001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e512079",
   "metadata": {},
   "source": [
    "We aiming to build this model:\n",
    "<img src=\"images/model.png\" style=\"width:800px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3492280",
   "metadata": {},
   "source": [
    "## 1 - Convolutional Neural Networks\n",
    "### 1.1 - Zero-Padding\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 1</b> </u><font color='purple'>  : <b>Zero-Padding</b><br> Image (3 channels, RGB) with a padding of 2. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f334e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 9, 9, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22bff604260>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 3)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1, 1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1, 1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0, :, :, 0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a97b0a",
   "metadata": {},
   "source": [
    "### 1.2 - Single Step of Convolution \n",
    "\n",
    "- Takes an input volume \n",
    "- Applies a filter at every position of the input\n",
    "- Outputs another volume (usually of different size)\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 2</b> </u><font color='purple'>  : <b>Convolution operation</b><br> with a filter of 3x3 and a stride of 1 (stride = amount you move the window each time you slide) </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee072c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898847e9",
   "metadata": {},
   "source": [
    "### 1.3 - Convolutional Neural Networks - Forward Pass\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722b20f",
   "metadata": {},
   "source": [
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Figure 3</b> </u><font color='purple'>  : <b>Definition of a slice using vertical and horizontal start/end (with a 2x2 filter)</b> <br> This figure shows only a single channel.  </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4b56f",
   "metadata": {},
   "source": [
    "**Reminder**:\n",
    "    \n",
    "The formulas relating the output shape of the convolution to the input shape are:\n",
    "    \n",
    "$$n_H = \\Bigl\\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_W = \\Bigl\\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_C = \\text{number of filters used in the convolution}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32a5793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.5511276474566768\n",
      "Z[0,2,1] =\n",
      " [-2.17796037  8.07171329 -0.5772704   3.36286738  4.48113645 -2.89198428\n",
      " 10.99288867  3.03171932]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 7, 4)\n",
    "W = np.random.randn(3, 3, 4, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "z_mean = np.mean(Z)\n",
    "z_0_2_1 = Z[0, 2, 1]\n",
    "cache_0_1_2_3 = cache_conv[0][1][2][3]\n",
    "print(\"Z's mean =\\n\", z_mean)\n",
    "print(\"Z[0,2,1] =\\n\", z_0_2_1)\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_0_1_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a35d1c",
   "metadata": {},
   "source": [
    "Finally, a CONV layer should also contain an activation (not done here yet):\n",
    "```python\n",
    "# Convolve the window to get back one output neuron\n",
    "Z[i, h, w, c] = ...\n",
    "# Apply activation\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c5e58",
   "metadata": {},
   "source": [
    "## 2 - Pooling Layer \n",
    "The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. The two types of pooling layers are: \n",
    "\n",
    "- Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.\n",
    "\n",
    "- Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters such as the window size $f$. This specifies the height and width of the $f \\times f$ window you would compute a *max* or *average* over. \n",
    "\n",
    "### 2.1 - Forward Pooling\n",
    "Now, we are going to implement MAX-POOL and AVG-POOL, in the same function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeaae8f",
   "metadata": {},
   "source": [
    "**Reminder**:\n",
    "As there's no padding, the formulas binding the output shape of the pooling to the input shape is:\n",
    "\n",
    "$$n_H = \\Bigl\\lfloor \\frac{n_{H_{prev}} - f}{stride} \\Bigr\\rfloor +1$$\n",
    "\n",
    "$$n_W = \\Bigl\\lfloor \\frac{n_{W_{prev}} - f}{stride} \\Bigr\\rfloor +1$$\n",
    "\n",
    "$$n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44be0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "\n",
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[1.96710175 0.84616065 1.27375593]\n",
      " [1.96710175 0.84616065 1.23616403]\n",
      " [1.62765075 1.12141771 1.2245077 ]]\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[ 0.44497696 -0.00261695 -0.31040307]\n",
      " [ 0.50811474 -0.23493734 -0.23961183]\n",
      " [ 0.11872677  0.17255229 -0.22112197]]\n",
      "\n",
      "\u001b[0mCASE 2:\n",
      "\n",
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[0] =\n",
      " [[[1.74481176 0.90159072 1.65980218]\n",
      "  [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      " [[1.13162939 1.51981682 2.18557541]\n",
      "  [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[1] =\n",
      " [[[-0.17313416  0.32377198 -0.34317572]\n",
      "  [ 0.02030094  0.14141479 -0.01231585]]\n",
      "\n",
      " [[ 0.42944926  0.08446996 -0.27290905]\n",
      "  [ 0.15077452  0.28911175  0.00123239]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "print(\"CASE 1:\\n\")\n",
    "np.random.seed(1)\n",
    "A_prev_case_1 = np.random.randn(2, 5, 5, 3)\n",
    "hparameters_case_1 = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "\n",
    "\n",
    "# Case 2: stride of 2\n",
    "print(\"\\n\\033[0mCASE 2:\\n\")\n",
    "np.random.seed(1)\n",
    "A_prev_case_2 = np.random.randn(2, 5, 5, 3)\n",
    "hparameters_case_2 = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[0] =\\n\", A[0])\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1] =\\n\", A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a3cf5",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**IMPORTANT TO REMEMBER**:\n",
    "\n",
    "* A convolution extracts features from an input image by taking the dot product between the input data and a 3D array of weights (the filter). \n",
    "* The 2D output of the convolution is called the feature map\n",
    "* A convolution layer is where the filter slides over the image and computes the dot product \n",
    "    * This transforms the input volume into an output volume of different size \n",
    "* Zero padding helps keep more information at the image borders, and is helpful for building deeper networks, because you can build a CONV layer without shrinking the height and width of the volumes\n",
    "* Pooling layers gradually reduce the height and width of the input by sliding a 2D window over each specified region, then summarizing the features in that region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944c199",
   "metadata": {},
   "source": [
    "## 3 - Backpropagation in Convolutional Neural Networks \n",
    "### 3.1 - Convolutional Layer Backward Pass \n",
    "\n",
    "Let's start by implementing the backward pass for a CONV layer. \n",
    "\n",
    "#### 3.1.1 - Computing dA:\n",
    "This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example:\n",
    "\n",
    "$$dA \\mathrel{+}= \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "Where $W_c$ is a filter and $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33499dff",
   "metadata": {},
   "source": [
    "#### 3.1.2 - Computing dW:\n",
    "This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss:\n",
    "\n",
    "$$dW_c  \\mathrel{+}= \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "Where $a_{slice}$ corresponds to the slice which was used to generate the activation $Z_{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice. Since it is the same $W$, we will just add up all such gradients to get $dW$. \n",
    "\n",
    "this formula translates into:\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05bfff",
   "metadata": {},
   "source": [
    "#### 3.1.3 - Computing db:\n",
    "\n",
    "This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:\n",
    "\n",
    "$$db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. \n",
    "\n",
    "this formula translates into:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b510b330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59daf5",
   "metadata": {},
   "source": [
    "## 3.2 Pooling Layer - Backward Pass\n",
    "\n",
    "Next, let's implement the backward pass for the pooling layer, starting with the MAX-POOL layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagate the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. \n",
    "\n",
    "### 3.2.1 Max Pooling - Backward Pass  \n",
    "\n",
    "Before jumping into the backpropagation of the pooling layer, we are going to build a helper function called `create_mask_from_window()` which does the following: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "As we can see, this function creates a \"mask\" matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0). We'll see later that the backward pass for average pooling is similar to this, but uses a different mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436248ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n",
      "x1 =  [[-1  2  3]\n",
      " [ 2 -3  2]\n",
      " [ 1  5 -2]]\n",
      "y =  [[False False False]\n",
      " [False False False]\n",
      " [False  True False]]\n",
      "mask1 =  [[False False False]\n",
      " [False False False]\n",
      " [False  True False]]\n",
      "mask1 - y =  [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2, 3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)\n",
    "\n",
    "x1 = np.array([[-1, 2, 3],\n",
    "              [2, -3, 2],\n",
    "              [1, 5, -2]])\n",
    "\n",
    "y = np.array([[False, False, False],\n",
    "     [False, False, False],\n",
    "     [False, True, False]])\n",
    "mask = create_mask_from_window(x1)\n",
    "print('x1 = ', x1)\n",
    "print('y = ', y)\n",
    "print('mask1 = ', mask)\n",
    "print(\"mask1 - y = \", mask.astype(float) - y.astype(float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc0bb2",
   "metadata": {},
   "source": [
    "Why keep track of the position of the max? It's because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will \"propagate\" the gradient back to this particular input value that had influenced the cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bb362",
   "metadata": {},
   "source": [
    "### 3.2.2 - Average Pooling - Backward Pass \n",
    "\n",
    "In max pooling, for each input window, all the \"influence\" on the output came from a single input value--the max. In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this.\n",
    "\n",
    "For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you'll use for the backward pass will look like: \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "This implies that each position in the $dZ$ matrix contributes equally to output because in the forward pass, we took an average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbb6024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2, 2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6b6d6",
   "metadata": {},
   "source": [
    "### 3.2.3 Putting it Together: Pooling Backward \n",
    "\n",
    "We now have everything we need to compute backward propagation on a pooling layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ebe986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 2, 2)\n",
      "(5, 5, 3, 2)\n",
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev1[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev2[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(A.shape)\n",
    "print(cache[0].shape)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev1 = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev1[1,1] = ', dA_prev1[1, 1])  \n",
    "print()\n",
    "dA_prev2 = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev2[1,1] = ', dA_prev2[1, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e7e88",
   "metadata": {},
   "source": [
    "**We now understand how convolutional neural networks work, and have implemented all the building blocks of a neural network.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
